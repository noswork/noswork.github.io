#!/usr/bin/env python3
"""
Copyright Â© 2025 nos
All Rights Reserved.

æœªç¶“æˆæ¬Šï¼Œç¦æ­¢ä½¿ç”¨ã€è¤‡è£½ã€ä¿®æ”¹æˆ–åˆ†ç™¼æœ¬ä»£ç¢¼ã€‚
Unauthorized copying, modification, or distribution is strictly prohibited.

Contact: discord: noswork
"""

"""
å¾å·´å“ˆå§†ç‰¹è«–å£‡æŠ“å–ä¸¦ä¸‹è¼‰ Tokyo Ghoul GIF åœ–ç‰‡
"""

import re
import os
import json
import time
from urllib.request import urlopen, Request, urlretrieve
from urllib.error import URLError, HTTPError
from html.parser import HTMLParser

class ImgurLinkParser(HTMLParser):
    """è§£æ HTML ä¸­çš„ Imgur é€£çµ"""
    def __init__(self):
        super().__init__()
        self.imgur_links = []
        
    def handle_starttag(self, tag, attrs):
        if tag == 'img':
            for attr, value in attrs:
                if attr == 'src' and 'imgur.com' in value:
                    # ç¢ºä¿æ˜¯ .gif æˆ–å¯ä»¥è½‰æ›ç‚º .gif
                    if '.gif' in value or '//' in value:
                        # è¦ç¯„åŒ– URL
                        if value.startswith('//'):
                            value = 'https:' + value
                        elif not value.startswith('http'):
                            value = 'https://' + value
                        
                        # ç¢ºä¿æ˜¯ GIF æ ¼å¼
                        if '.gif' in value or 'i.imgur.com' in value:
                            self.imgur_links.append(value)

def fetch_page(url, retries=3, delay=2):
    """æŠ“å–ç¶²é å…§å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(retries):
        try:
            req = Request(url, headers=headers)
            with urlopen(req, timeout=10) as response:
                return response.read().decode('utf-8', errors='ignore')
        except (URLError, HTTPError) as e:
            print(f"âŒ æŠ“å–å¤±æ•— (å˜—è©¦ {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay)
    
    return None

def extract_imgur_gifs(html_content):
    """å¾ HTML ä¸­æå– Imgur GIF é€£çµ"""
    if not html_content:
        return []
    
    parser = ImgurLinkParser()
    try:
        parser.feed(html_content)
    except Exception as e:
        print(f"âŒ è§£æ HTML æ™‚å‡ºéŒ¯: {e}")
    
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ä½œç‚ºå‚™ä»½æ–¹æ³•
    regex_pattern = r'https?://i\.imgur\.com/[a-zA-Z0-9]+\.gif'
    regex_matches = re.findall(regex_pattern, html_content)
    
    # åˆä½µå…©ç¨®æ–¹æ³•çš„çµæœ
    all_links = list(set(parser.imgur_links + regex_matches))
    
    # ç¢ºä¿æ‰€æœ‰é€£çµéƒ½æ˜¯ .gif æ ¼å¼
    gif_links = []
    for link in all_links:
        if '.gif' in link:
            gif_links.append(link)
        elif 'i.imgur.com' in link and not any(ext in link for ext in ['.jpg', '.jpeg', '.png']):
            # å¦‚æœæ˜¯ imgur é€£çµä½†æ²’æœ‰å‰¯æª”åï¼Œå˜—è©¦æ·»åŠ  .gif
            if not link.endswith('.gif'):
                gif_links.append(link + '.gif' if '?' not in link else link.split('?')[0] + '.gif')
            else:
                gif_links.append(link)
    
    return list(set(gif_links))  # å»é‡

def download_gif(url, output_dir, filename=None):
    """ä¸‹è¼‰å–®å€‹ GIF æª”æ¡ˆ"""
    try:
        if filename is None:
            # å¾ URL ä¸­æå–æª”å
            filename = url.split('/')[-1].split('?')[0]
            if not filename.endswith('.gif'):
                filename += '.gif'
        
        output_path = os.path.join(output_dir, filename)
        
        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³é
        if os.path.exists(output_path):
            print(f"â­ï¸  å·²å­˜åœ¨: {filename}")
            return True
        
        # ä¸‹è¼‰æª”æ¡ˆ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        req = Request(url, headers=headers)
        
        print(f"â¬‡ï¸  ä¸‹è¼‰ä¸­: {filename}")
        with urlopen(req, timeout=30) as response:
            with open(output_path, 'wb') as f:
                f.write(response.read())
        
        print(f"âœ… å®Œæˆ: {filename}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•— ({filename}): {e}")
        return False

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ¬ Tokyo Ghoul GIF ä¸‹è¼‰å·¥å…·")
    print("=" * 50)
    
    # ç›®æ¨™ URL
    urls = [
        "https://forum.gamer.com.tw/C.php?bsn=45101&snA=228",
        "https://forum.gamer.com.tw/C.php?page=2&bsn=45101&snA=228"
    ]
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'assets', 'loading-gifs')
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    print()
    
    # æ”¶é›†æ‰€æœ‰ GIF é€£çµ
    all_gif_links = []
    
    for i, url in enumerate(urls, 1):
        print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {i} é ...")
        html = fetch_page(url)
        
        if html:
            gif_links = extract_imgur_gifs(html)
            print(f"âœ… æ‰¾åˆ° {len(gif_links)} å€‹ GIF é€£çµ")
            all_gif_links.extend(gif_links)
        else:
            print(f"âŒ ç„¡æ³•æŠ“å–ç¬¬ {i} é ")
        
        print()
        time.sleep(1)  # é¿å…è«‹æ±‚éå¿«
    
    # å»é‡
    all_gif_links = list(set(all_gif_links))
    print(f"ğŸ¯ ç¸½å…±æ‰¾åˆ° {len(all_gif_links)} å€‹å”¯ä¸€çš„ GIF é€£çµ")
    print()
    
    # ä¿å­˜é€£çµåˆ—è¡¨åˆ° JSON
    json_path = os.path.join(project_root, 'data', 'tokyo-ghoul-gifs.json')
    os.makedirs(os.path.dirname(json_path), exist_ok=True)
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_gif_links, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ é€£çµåˆ—è¡¨å·²ä¿å­˜: {json_path}")
    print()
    
    # ä¸‹è¼‰æ‰€æœ‰ GIF
    print(f"â¬‡ï¸  é–‹å§‹ä¸‹è¼‰ {len(all_gif_links)} å€‹ GIF...")
    print()
    
    success_count = 0
    fail_count = 0
    
    for i, url in enumerate(all_gif_links, 1):
        print(f"[{i}/{len(all_gif_links)}]", end=" ")
        
        if download_gif(url, output_dir):
            success_count += 1
        else:
            fail_count += 1
        
        # é¿å…è«‹æ±‚éå¿«
        if i % 10 == 0:
            time.sleep(2)
        else:
            time.sleep(0.5)
    
    print()
    print("=" * 50)
    print(f"âœ… æˆåŠŸ: {success_count} å€‹")
    print(f"âŒ å¤±æ•—: {fail_count} å€‹")
    print(f"ğŸ“ GIF ä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“„ é€£çµåˆ—è¡¨: {json_path}")
    
    # å‰µå»ºç´¢å¼•æ–‡ä»¶
    downloaded_files = [f for f in os.listdir(output_dir) if f.endswith('.gif')]
    index_path = os.path.join(project_root, 'data', 'gif-index.json')
    
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(sorted(downloaded_files), f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶å·²å‰µå»º: {index_path}")
    print(f"ğŸ¬ å…±æœ‰ {len(downloaded_files)} å€‹ GIF å¯ç”¨")

if __name__ == '__main__':
    main()

